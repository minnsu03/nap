{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBb1YgnhMbHL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(320)\n",
    "x_train = np.linspace(-1, 1, 50)\n",
    "f = lambda x: 0.5 * x + 1.0\n",
    "y_train = f(x_train) + 0.4 * np.random.rand(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjv0PnEXMdl6"
   },
   "outputs": [],
   "source": [
    "# 손실함수\n",
    "def loss(w, x_set, y_set):\n",
    "    N = len(x_set)\n",
    "    val = 0.0\n",
    "    for i in range(len(x_set)):\n",
    "        val += 0.5 * ( w[0] * x_set[i] + w[1] - y_set[i] )**2\n",
    "    return val / N\n",
    "\n",
    "#손실함수의 그래디언트\n",
    "def loss_grad(w, x_set, y_set):\n",
    "    N = len(x_set)\n",
    "    val = np.zeros(len(w))\n",
    "    for i in range(len(x_set)):\n",
    "        er = w[0] * x_set[i] + w[1] - y_set[i]\n",
    "        val += er * np.array([x_set[i], 1.0])\n",
    "    return val / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5xo8z_V8Mekt"
   },
   "outputs": [],
   "source": [
    "def generate_batches(batch_size, features, labels):\n",
    "    outout_batches = []\n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        outout_batches.append(batch)\n",
    "    return outout_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fHq3CmfwMkHc",
    "outputId": "a0249729-2805-439d-f16d-082d13287429"
   },
   "outputs": [],
   "source": [
    "# SGD\n",
    "batch_size = 5 # 배치 크기\n",
    "lr = 0.1 # 학습률\n",
    "MaxEpochs = 10 # 반복 횟수\n",
    "\n",
    "paths = []\n",
    "batch_loss = []\n",
    "w0 = np.array([4.0, -1.0]) # 1) 초깃값\n",
    "search_direction = np.zeros_like(w0)\n",
    "\n",
    "# 2) 데이터 셔플링\n",
    "np.random.seed(320)\n",
    "idx = np.arange(len(x_train))\n",
    "np.random.shuffle(idx)\n",
    "shuffled_x_train = x_train[idx]\n",
    "shuffled_y_train = y_train[idx]\n",
    "\n",
    "# 알고리즘\n",
    "for epoch in range(MaxEpochs+1): # 5) MaxEpochs번 반복\n",
    "    for x_batch, y_batch in generate_batches(batch_size, shuffled_x_train, shuffled_y_train): # 3) 미니 배치 생성\n",
    "        paths.append(w0)\n",
    "        batch_loss.append(loss(w0, x_batch, y_batch))\n",
    "        grad = loss_grad(w0, x_batch, y_batch) # 4)-1 미니 치에서 그래디언트 계산\n",
    "        search_direction = -grad # 4)-2 탐색 방향 설정\n",
    "        lr = lr # 4)-3 학습률 설정\n",
    "        # 4)-4 파라미터 업데이트\n",
    "        dw = lr * search_direction\n",
    "        w0 = w0 + dw\n",
    "    print('{:02d}\\t{}\\t{:5.4f}'.format(epoch, w0, loss(w0, x_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "f7-y7fDTMlyR",
    "outputId": "22e5fafc-9713-4c83-b5b1-326b8540b185"
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.pyplot as plt\n",
    "def contour_with_path(l, x, y, paths, norm=LogNorm(),level=np.logspace(0, 5, 35), minima=None):\n",
    "    paths = np.array(paths).T\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    ax.contour(x, y, l, levels=level, norm=norm, cmap=plt.cm.jet)\n",
    "    ax.quiver(paths[0, :-1], paths[1, :-1], paths[0, 1:]-paths[0, :-1], paths[1, 1:]-paths[1, :-1],\n",
    "        scale_units='xy', angles='xy', scale=1, color='k')\n",
    "    if minima is not None:\n",
    "        ax.plot(*minima, 'r*', markersize=18)\n",
    "\n",
    "    ax.set_xlabel('$a$')\n",
    "    ax.set_ylabel('$b$')\n",
    "    plt.show()\n",
    "\n",
    "W0 = np.linspace(-5, 7, 101)\n",
    "W1 = np.linspace(-2, 5, 101)\n",
    "W0, W1 = np.meshgrid(W0, W1)\n",
    "LOSSW = W0 * 0\n",
    "for i in range(W0.shape[0]):\n",
    "    for j in range(W0.shape[1]):\n",
    "        wij = np.array([W0[i, j], W1[i, j]])\n",
    "        LOSSW[i, j] = loss(wij, x_train, y_train)\n",
    "\n",
    "contour_with_path(LOSSW, W0, W1, paths, norm=None, level=np.linspace(0, 10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "nsVUlzGWMmyQ",
    "outputId": "d4558e4d-8f23-483c-bff8-54f8e47a0395"
   },
   "outputs": [],
   "source": [
    "plt.plot(batch_loss, '.-k', markerfacecolor='none')\n",
    "plt.grid()\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('loss')\n",
    "plt.title('loss on a batch by SGD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "VnGURgKKMnva",
    "outputId": "0f8d9f4e-cf48-4239-b217-7a5c29b9f298"
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "def visualize_l2(w, b, x_train, y_train, loss):\n",
    "    from matplotlib.patches import Rectangle\n",
    "    loss_name = 'loss'\n",
    "    plt.plot(x_train, y_train, '.k', markerfacecolor='none')\n",
    "    plt.plot(x_train, w * x_train + b, '--k')\n",
    "    currentAxis = plt.gca()\n",
    "    for xx,yy in zip(x_train, y_train):\n",
    "        currentAxis.add_patch(Rectangle((xx, yy),\n",
    "                                        w * xx + b - yy, w * xx + b - yy,\n",
    "                              alpha=0.1, facecolor='gray', edgecolor='k'))\n",
    "    plt.grid()\n",
    "    plt.axis('equal')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(\"a={:1.2f}, b={:1.2f}(loss={:5.4f})\".format(w, b, loss))\n",
    "\n",
    "visualize_l2(w0[0], w0[1], x_train, y_train, loss(w0, x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Fg-Mk9qPMpr4",
    "outputId": "a9dc8e5e-113b-47c9-ba4e-497a5485b44e"
   },
   "outputs": [],
   "source": [
    "# Momentum\n",
    "batch_size = 5 # 배치 크기\n",
    "epsilon = 0.03 # 학습률\n",
    "MaxEpochs = 10 # 반복 횟수\n",
    "\n",
    "w0 = np.array([4.0, -1.0]) # 1) 초깃값\n",
    "paths = []\n",
    "batch_loss = []\n",
    "alpha = 0.9\n",
    "\n",
    "v = np.zeros_like(w0)\n",
    "\n",
    "for epoch in range(MaxEpochs + 1):\n",
    "    for x_batch, y_batch in generate_batches(batch_size, shuffled_x_train, shuffled_y_train):\n",
    "        paths.append(w0)\n",
    "        batch_loss.append(loss(w0, x_batch, y_batch))\n",
    "        grad = loss_grad(w0, x_batch, y_batch)\n",
    "\n",
    "        v = alpha * v - epsilon * grad\n",
    "        w0 = w0 + v\n",
    "\n",
    "    print('{:02d}\\t{}\\t{:5.4f}'.format(epoch, w0, loss(w0, x_train, y_train)))\n",
    "\n",
    "W0 = np.linspace(-5, 7, 101)\n",
    "W1 = np.linspace(-2, 5, 101)\n",
    "W0, W1 = np.meshgrid(W0, W1)\n",
    "LOSSW = W0 * 0\n",
    "for i in range(W0.shape[0]):\n",
    "    for j in range(W0.shape[1]):\n",
    "        wij = np.array([W0[i, j], W1[i, j]])\n",
    "        LOSSW[i, j] = loss(wij, x_train, y_train)\n",
    "\n",
    "contour_with_path(LOSSW, W0, W1, paths, norm=None, level=np.linspace(0, 10, 10))\n",
    "plt.plot(batch_loss, '.-k', markerfacecolor='none')\n",
    "plt.grid()\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('loss')\n",
    "plt.title('loss on a batch by Momentum')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a-nDMONINFOE",
    "outputId": "e3377f0d-5a40-419d-b6c0-0f17c6a02e16"
   },
   "outputs": [],
   "source": [
    "# Nesterov\n",
    "batch_size = 5 # 뱃치 크기\n",
    "epsilon = 0.03 # 학습율\n",
    "MaxEpochs = 10 # 반복 횟수\n",
    "\n",
    "w0 = np.array([4.0, -1.0]) # 1) 초기값\n",
    "paths = []\n",
    "batch_loss = []\n",
    "alpha = 0.9\n",
    "\n",
    "v = np.zeros_like(w0)\n",
    "\n",
    "for epoch in range(MaxEpochs + 1):\n",
    "    for x_batch, y_batch in generate_batches(batch_size, shuffled_x_train, shuffled_y_train):\n",
    "        paths.append(w0)\n",
    "        batch_loss.append(loss(w0, x_batch, y_batch))\n",
    "\n",
    "        grad = loss_grad(w0 + alpha * v , x_batch, y_batch)\n",
    "\n",
    "        v = alpha * v - epsilon * grad\n",
    "        w0 = w0 + v\n",
    "\n",
    "    print('{:02d}\\t{}\\t{:5.4f}'.format(epoch, w0, loss(w0, x_train, y_train)))\n",
    "\n",
    "\n",
    "W0 = np.linspace(-5, 7, 101)\n",
    "W1 = np.linspace(-2, 5, 101)\n",
    "W0, W1 = np.meshgrid(W0, W1)\n",
    "LOSSW = W0 * 0\n",
    "for i in range(W0.shape[0]):\n",
    "    for j in range(W0.shape[1]):\n",
    "        wij = np.array([W0[i, j], W1[i, j]])\n",
    "        LOSSW[i, j] = loss(wij, x_train, y_train)\n",
    "\n",
    "contour_with_path(LOSSW, W0, W1, paths, norm=None, level=np.linspace(0, 10, 10))\n",
    "plt.plot(batch_loss, '.-k', markerfacecolor='none')\n",
    "plt.grid()\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('loss')\n",
    "plt.title('loss on a batch by Nesterov')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "am8j0aViNLxr",
    "outputId": "633865ed-1558-4338-9699-e5147da98000"
   },
   "outputs": [],
   "source": [
    "# Adagrad\n",
    "batch_size = 5 # 배치 크기\n",
    "MaxEpochs = 10 # 반복 횟수\n",
    "\n",
    "w0 = np.array([2.0, 4.0]) # 1) 초깃값\n",
    "epsilon = 1.0\n",
    "delta = 1E-7\n",
    "r = np.zeros_like(w0)\n",
    "\n",
    "paths = []\n",
    "batch_loss = []\n",
    "\n",
    "\n",
    "for epoch in range(MaxEpochs + 1):\n",
    "    for x_batch, y_batch in generate_batches(batch_size, shuffled_x_train, shuffled_y_train):\n",
    "        paths.append(w0)\n",
    "        batch_loss.append(loss(w0, x_batch, y_batch))\n",
    "\n",
    "        grad = loss_grad(w0, x_batch, y_batch)\n",
    "        r += grad ** 2\n",
    "\n",
    "        adjusted_lr = epsilon / (np.sqrt(r) + delta)\n",
    "        w0 = w0 - adjusted_lr * grad\n",
    "\n",
    "    print('{:02d}\\t{}\\t{:5.4f}'.format(epoch, w0, loss(w0, x_train, y_train)))\n",
    "\n",
    "\n",
    "W0 = np.linspace(-5, 7, 101)\n",
    "W1 = np.linspace(-2, 5, 101)\n",
    "W0, W1 = np.meshgrid(W0, W1)\n",
    "LOSSW = W0 * 0\n",
    "for i in range(W0.shape[0]):\n",
    "    for j in range(W0.shape[1]):\n",
    "        wij = np.array([W0[i, j], W1[i, j]])\n",
    "        LOSSW[i, j] = loss(wij, x_train, y_train)\n",
    "\n",
    "contour_with_path(LOSSW, W0, W1, paths, norm=None, level=np.linspace(0, 10, 10))\n",
    "plt.plot(batch_loss, '.-k', markerfacecolor='none')\n",
    "plt.grid()\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('loss')\n",
    "plt.title('loss on a batch by Adagrad')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Y7QcD72ZNOjZ",
    "outputId": "721e3742-e63d-4412-ff12-8b4a129b619b"
   },
   "outputs": [],
   "source": [
    "# RMSProp\n",
    "batch_size = 5 # 배치 크기\n",
    "MaxEpochs = 10 # 반복 횟수\n",
    "\n",
    "w0 = np.array([2.0, 4.0]) # 1) 초깃값\n",
    "epsilon = 0.25\n",
    "delta = 1E-10\n",
    "rho = 0.9\n",
    "r = np.zeros_like(w0)\n",
    "\n",
    "paths = []\n",
    "batch_loss = []\n",
    "\n",
    "for epoch in range(MaxEpochs + 1):\n",
    "    for x_batch, y_batch in generate_batches(batch_size, shuffled_x_train, shuffled_y_train):\n",
    "        paths.append(w0)\n",
    "        batch_loss.append(loss(w0, x_batch, y_batch))\n",
    "\n",
    "        grad = loss_grad(w0, x_batch, y_batch)\n",
    "        r = rho * r + (1 - rho) * grad ** 2\n",
    "\n",
    "        adjusted_lr = epsilon / (np.sqrt(r) + delta)\n",
    "        w0 = w0 - adjusted_lr * grad\n",
    "\n",
    "    print('{:02d}\\t{}\\t{:5.4f}'.format(epoch, w0, loss(w0, x_train, y_train)))\n",
    "\n",
    "\n",
    "W0 = np.linspace(-5, 7, 101)\n",
    "W1 = np.linspace(-2, 5, 101)\n",
    "W0, W1 = np.meshgrid(W0, W1)\n",
    "LOSSW = W0 * 0\n",
    "for i in range(W0.shape[0]):\n",
    "    for j in range(W0.shape[1]):\n",
    "        wij = np.array([W0[i, j], W1[i, j]])\n",
    "        LOSSW[i, j] = loss(wij, x_train, y_train)\n",
    "\n",
    "contour_with_path(LOSSW, W0, W1, paths, norm=None, level=np.linspace(0, 10, 10))\n",
    "plt.plot(batch_loss, '.-k', markerfacecolor='none')\n",
    "plt.grid()\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('loss')\n",
    "plt.title('loss on a batch by RMSProp')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bsKxbWuWNQUQ",
    "outputId": "8072647b-125c-4bdc-c43b-9b3cf2285a5a"
   },
   "outputs": [],
   "source": [
    "# Adam\n",
    "batch_size = 5 # 배치 크기\n",
    "MaxEpochs = 10 # 반복 횟수\n",
    "\n",
    "w0 = np.array([2.0, 4.0]) # 1) 초깃값\n",
    "epsilon = 1.0\n",
    "delta = 1E-8\n",
    "rho1 = 0.9\n",
    "rho2 = 0.999\n",
    "s = np.zeros_like(w0)\n",
    "r = np.zeros_like(w0)\n",
    "t = 0\n",
    "paths = []\n",
    "batch_loss = []\n",
    "\n",
    "for epoch in range(MaxEpochs + 1):\n",
    "    for x_batch, y_batch in generate_batches(batch_size, shuffled_x_train, shuffled_y_train):\n",
    "        t+=1\n",
    "        paths.append(w0)\n",
    "        batch_loss.append(loss(w0, x_batch, y_batch))\n",
    "\n",
    "        grad = loss_grad(w0, x_batch, y_batch)\n",
    "        s = rho1 * s + (1 - rho1) * grad\n",
    "        r = rho2 * r + (1 - rho2) * (grad ** 2)\n",
    "\n",
    "        s_hat = s / (1 - rho1 ** (t))\n",
    "        r_hat = r / (1 - rho2 ** (t))\n",
    "\n",
    "        w0 = w0 - epsilon * s_hat / (np.sqrt(r_hat) + delta)\n",
    "\n",
    "    print('{:02d}\\t{}\\t{:5.4f}'.format(epoch, w0, loss(w0, x_train, y_train)))\n",
    "\n",
    "W0 = np.linspace(-5, 7, 101)\n",
    "W1 = np.linspace(-2, 5, 101)\n",
    "W0, W1 = np.meshgrid(W0, W1)\n",
    "LOSSW = W0 * 0\n",
    "for i in range(W0.shape[0]):\n",
    "    for j in range(W0.shape[1]):\n",
    "        wij = np.array([W0[i, j], W1[i, j]])\n",
    "        LOSSW[i, j] = loss(wij, x_train, y_train)\n",
    "\n",
    "contour_with_path(LOSSW, W0, W1, paths, norm=None, level=np.linspace(0, 10, 10))\n",
    "plt.plot(batch_loss, '.-k', markerfacecolor='none')\n",
    "plt.grid()\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('loss')\n",
    "plt.title('loss on a batch by Adam')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0bAQ7aaNeZe"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "np.random.seed(327)\n",
    "x_train = np.linspace(-1,1,50)\n",
    "y_train = 0.25 * np.cos(np.pi * x_train) + 0.3 * np.sin(np.pi * x_train) + 0.2 * (2 * np.random.rand(len(x_train)) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "pGL7f3LRNhFt",
    "outputId": "955af5c1-d618-426d-8c8d-4504313e9539"
   },
   "outputs": [],
   "source": [
    "plt.plot(x_train, y_train, '.k')\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZJe-N81xNhmW",
    "outputId": "b35b2174-9a09-4e66-fb95-c621e80b2782"
   },
   "outputs": [],
   "source": [
    "# 다항함수용 함수들\n",
    "def loss(w, x_set, y_set):\n",
    "    N = len(x_set)\n",
    "    val = 0.0\n",
    "    for i in range(len(x_set)):\n",
    "        y_pred = np.polyval(w, x_set[i])\n",
    "        val += 0.5 * (y_pred - y_set[i])**2\n",
    "    return val / N\n",
    "\n",
    "# 손실 함수의 그래디언트\n",
    "def loss_grad(w, x_set, y_set):\n",
    "    N = len(x_set)\n",
    "    grad = np.zeros(len(w))\n",
    "    for i in range(len(x_set)):\n",
    "        y_pred = np.polyval(w, x_set[i])\n",
    "        err = y_pred - y_set[i]\n",
    "        x_powers = np.array([x_set[i]**p for p in range(len(w) - 1, -1, -1)])\n",
    "        grad += err * x_powers\n",
    "    return grad / N\n",
    "\n",
    "# 미니 배치 생성 함수\n",
    "def generate_batches(batch_size, features, labels):\n",
    "    output_batches = []\n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        output_batches.append(batch)\n",
    "    return output_batches\n",
    "# 플로팅\n",
    "def plot_regression(x_train, y_train, w):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    plt.plot(x_train, y_train, '.k', label='Training data')\n",
    "    x_fit = np.linspace(-1, 1, 200)\n",
    "    y_fit = np.polyval(w, x_fit)\n",
    "    plt.plot(x_fit, y_fit, '-r', label='Regression result')\n",
    "\n",
    "    plt.grid()\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('4th Degree Polynomial Regression')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# \"Adam\"\n",
    "batch_size = 10\n",
    "lr = 0.1\n",
    "MaxEpochs = 100\n",
    "\n",
    "w0 = np.ones(5)  # 4차 다항식 초기값\n",
    "\n",
    "np.random.seed(916)\n",
    "idx = np.arange(len(x_train))\n",
    "np.random.shuffle(idx)\n",
    "shuffled_x_train = x_train[idx]\n",
    "shuffled_y_train = y_train[idx]\n",
    "\n",
    "epsilon = 1.0\n",
    "delta = 1E-8\n",
    "rho1 = 0.9\n",
    "rho2 = 0.999\n",
    "s = np.zeros_like(w0)\n",
    "r = np.zeros_like(w0)\n",
    "t = 0\n",
    "paths = []\n",
    "batch_loss = []\n",
    "\n",
    "for epoch in range(MaxEpochs + 1):\n",
    "    for x_batch, y_batch in generate_batches(batch_size, shuffled_x_train, shuffled_y_train):\n",
    "        paths.append(w0)\n",
    "        batch_loss.append(loss(w0, x_batch, y_batch))\n",
    "\n",
    "        grad = loss_grad(w0, x_batch, y_batch)\n",
    "        s = rho1 * s + (1 - rho1) * grad\n",
    "        r = rho2 * r + (1 - rho2) * grad ** 2\n",
    "\n",
    "        s_hat = s / (1 - rho1 ** (epoch + 1))\n",
    "        r_hat = r / (1 - rho2 ** (epoch + 1))\n",
    "\n",
    "        w0 = w0 - epsilon * s_hat / (np.sqrt(r_hat) + delta)\n",
    "    if epoch%10==0:\n",
    "      print('{:02d}\\t{}\\t{:5.4f}'.format(epoch, w0, loss(w0, x_train, y_train)))\n",
    "\n",
    "\n",
    "# 최종결과 플로팅\n",
    "plot_regression(x_train, y_train, w0)\n",
    "# loss curve 플로팅\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(batch_loss, label=\"Adam Loss Curve\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curve for Adagrad Optimization\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Vzop5udsK6Dt",
    "outputId": "49ea898a-fe45-41ef-ebde-06d5220af248"
   },
   "outputs": [],
   "source": [
    "# \"Adagrad\"\n",
    "\n",
    "batch_size = 10\n",
    "lr = 0.1\n",
    "MaxEpochs = 100\n",
    "\n",
    "w0 = np.ones(5)  # 4차 다항식 초기값\n",
    "\n",
    "np.random.seed(916)\n",
    "idx = np.arange(len(x_train))\n",
    "np.random.shuffle(idx)\n",
    "shuffled_x_train = x_train[idx]\n",
    "shuffled_y_train = y_train[idx]\n",
    "\n",
    "epsilon = 1.0\n",
    "delta = 1E-8\n",
    "rho1 = 0.9\n",
    "rho2 = 0.999\n",
    "s = np.zeros_like(w0)\n",
    "r = np.zeros_like(w0)\n",
    "t = 0\n",
    "paths = []\n",
    "batch_loss = []\n",
    "\n",
    "for epoch in range(MaxEpochs + 1):\n",
    "    for x_batch, y_batch in generate_batches(batch_size, shuffled_x_train, shuffled_y_train):\n",
    "         paths.append(w0)\n",
    "         batch_loss.append(loss(w0, x_batch, y_batch))\n",
    "\n",
    "         grad = loss_grad(w0, x_batch, y_batch)\n",
    "         r += grad ** 2\n",
    "\n",
    "         adjusted_lr = epsilon / (np.sqrt(r) + delta)\n",
    "         w0 = w0 - adjusted_lr * grad\n",
    "\n",
    "    if epoch%10==0:\n",
    "      print('{:02d}\\t{}\\t{:5.4f}'.format(epoch, w0, loss(w0, x_train, y_train)))\n",
    "\n",
    "#최종결과 플로팅\n",
    "plot_regression(x_train, y_train, w0)\n",
    "#loss curve 플로팅\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(batch_loss, label=\"Adagrad Loss Curve\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curve for Adagrad Optimization\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
